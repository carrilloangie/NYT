{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports: DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% (2 of 1855658) |             | Elapsed Time: 0:00:00 ETA:  1 day, 18:19:47"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path generation: DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (1855658 of 1855658) |#############| Elapsed Time: 23:14:53 Time: 23:14:53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction: DONE\n",
      "Delete empty rows: DONE\n"
     ]
    }
   ],
   "source": [
    "###All the imports\n",
    "import Organizado as org\n",
    "from Organizado import *\n",
    "\n",
    "print(\"All imports: DONE\")\n",
    "\n",
    "###Generate a list with all the paths of the documents\n",
    "path =\"./data/\"\n",
    "allArticles=org.GetPaths(path)\n",
    "print(\"Path generation: DONE\")\n",
    "\n",
    "###Extract body and title for each document\n",
    "ft,titles=org.Extraction(allArticles)\n",
    "print(\"Extraction: DONE\")\n",
    "\n",
    "###Delete empty data\n",
    "ft,titles,allArticles=Delete_empty(ft,titles,allArticles)\n",
    "print(\"Delete empty rows: DONE\")\n",
    "\n",
    "###Save Paths\n",
    "PathName=\"Mapping.bin\"\n",
    "org.SaveBinary(PathName,allArticles)\n",
    "print(\"Binary DONE\")\n",
    "\n",
    "###Save the bodies extracted\n",
    "BodyName=\"Full_Text.bin\"\n",
    "org.SaveBinary(BodyName,ft)\n",
    "###Save the titles extracted\n",
    "TitlesName=\"Titles_.bin\"\n",
    "org.SaveBinary(TitlesName,titles)\n",
    "print(\"Save the data: DONE\")\n",
    "\n",
    "###Tokenize the body\n",
    "tokens_body=org.Tokenize(ft)\n",
    "print(\"Tokenization: DONE\")\n",
    "\n",
    "###Normalize the body\n",
    "normal_body=org.Normalize(tokens_body)\n",
    "print(\"Normalization: DONE\")\n",
    "\n",
    "###Remove stopwords from the body\n",
    "noSW_body=org.removeSW(normal_body)\n",
    "print(\"Stopwords removal: DONE\")\n",
    "\n",
    "###Create the index TermID,Term\n",
    "terms_id=org.TermsID(noSW_body)\n",
    "print(\"Term Index creation: DONE\")\n",
    "\n",
    "###Save the index termID\n",
    "ID_name=\"Lex.bin\"\n",
    "org.SaveBinary(ID_name,terms_id)\n",
    "print(\"Save the data: DONE\")\n",
    "\n",
    "###Replace the terms in the document for the correpsonding ID\n",
    "body_term_ID=org.ReplaceID(noSW_body,terms_id)\n",
    "print(\"TermID replacing in document: DONE\")\n",
    "\n",
    "###Save the documents (body) into csv (dictionary)\n",
    "Body_PP_Name=\"Body_ID.bin\"\n",
    "org.SaveBinary(Body_PP_Name,body_term_ID)\n",
    "print(\"Save the data: DONE\")\n",
    "\n",
    "\n",
    "###Create the doc,termID pairs and merge multiple instances of the same term from the same document\n",
    "merged_body=org.merge(body_term_ID)\n",
    "print(\"Merge: DONE\")\n",
    "\n",
    "###Instances of the same term are then grouped and the result is split into a dictionary and postings.\n",
    "##Postings Lists\n",
    "postings_lists_body=org.PostingsLists(merged_body)\n",
    "print(\"Postings lists: DONE\")\n",
    "\n",
    "###Save the postings lists\n",
    "postings_lists_body_name='Postings_Lists_body.bin'\n",
    "org.SaveBinary(postings_lists_body_name,postings_lists_body)\n",
    "print(\"Save the data: DONE\")\n",
    "\n",
    "###Document Frequency\n",
    "doc_freq_body=org.doc_frequency(postings_lists_body)\n",
    "print(\"Document Frequency: DONE\")\n",
    "\n",
    "###Save the document frequency\n",
    "doc_freq_body_name='Document_Frequency_body.bin'\n",
    "org.SaveBinary(doc_freq_body_name,doc_freq_body)\n",
    "print(\"Save the data: DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[1,2,3,4,5,6,7,8,9,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=random.sample(a, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_id=[3,3,3,3,3,2,1,5,6,7,7,8,8,5,5,4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=list(set(term_id)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 3, 2, 1, 5, 6, 7, 7, 8, 8, 5, 5, 4, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
